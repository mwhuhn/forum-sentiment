{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Forum Data\n",
    "This notebook scrapes the Youbemom special needs forum and inserts the results into a SQLite database\n",
    "\n",
    "## Data Sources\n",
    "- Youbemom forum: https://www.youbemom.com/forum/special-needs\n",
    "\n",
    "## Changes\n",
    "- 2020-08-11: Started project\n",
    "- 2020-08-18: Updated forum crawl\n",
    "- 2020-08-22: Updated database structure\n",
    "- 2020-10-22: Added additional subforums\n",
    "- 2020-10-24: Added deleted post column\n",
    "\n",
    "## Database Structure\n",
    "- threads\n",
    " - id: automatically assigned\n",
    " - url: url of top post\n",
    " - subforum: subforum name\n",
    "- posts\n",
    " - id: automatically assigned\n",
    " - family_id: thread->id\n",
    " - message_id: the unique id of the message from the html\n",
    " - parent_id: id of post this post is responding to, 0 if top post\n",
    " - date_recorded: date the data is fetched\n",
    " - date_created: date the data was created\n",
    " - title: title of the post\n",
    " - body: body of the post\n",
    " - subforum: subforum name\n",
    "\n",
    "## TODO\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from pathlib import Path\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "For accessing the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by the db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "    except Error as err:\n",
    "        print(err)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_db(conn):\n",
    "    \"\"\" if the database exists, drop it and create a\n",
    "        SQLite database for the results\n",
    "    :param conn: database connection\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.executescript('''\n",
    "        CREATE TABLE IF NOT EXISTS threads (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "            url TEXT,\n",
    "            subforum TEXT\n",
    "        );\n",
    "        CREATE TABLE IF NOT EXISTS posts (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "            family_id INTEGER,\n",
    "            message_id TEXT,\n",
    "            parent_id INTEGER,\n",
    "            date_recorded TEXT,\n",
    "            date_created TEXT,\n",
    "            title TEXT,\n",
    "            body TEXT,\n",
    "            subforum TEXT,\n",
    "            deleted INTEGER\n",
    "        );\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_threads(conn, url, subforum):\n",
    "    \"\"\" inserts the parsed data into the threads table\n",
    "    :param parsed: a tuple of the parsed data\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    sql = ''' INSERT INTO threads(url, subforum)\n",
    "    VALUES(?,?) '''\n",
    "    parsed = (url, subforum)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_not_in_threads(conn, url):\n",
    "    \"\"\" checks to see if scraped url is already \n",
    "        in the threads database\n",
    "    :return: True if not scraped, False if already scraped\n",
    "    \"\"\"\n",
    "    sql = ''' SELECT id FROM threads WHERE url=?'''\n",
    "    parsed = (url,)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    result = cur.fetchone()\n",
    "    if result:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_posts(parsed, conn):\n",
    "    \"\"\" inserts the parsed data into the posts table\n",
    "    :param parsed: a tuple of the parsed data\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    sql = ''' INSERT INTO posts(family_id,message_id,parent_id,date_recorded,date_created,title,body,subforum,deleted)\n",
    "    VALUES(?,?,?,?,?,?,?,?,?) '''\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_not_in_posts(conn, message_id, deleted):\n",
    "    \"\"\" checks to see if the message_id is already \n",
    "        in the posts database, used to not overwrite\n",
    "        posts with the deleted message when re-running\n",
    "        the scraping of the posts\n",
    "    :return: True if not scraped, False if already scraped\n",
    "    \"\"\"\n",
    "    sql = ''' SELECT id, deleted FROM posts WHERE message_id=? '''\n",
    "    parsed = (message_id, )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    result = cur.fetchone()\n",
    "    if result:\n",
    "        if result[1] != deleted: # deleted after first scrape\n",
    "            sql = ''' UPDATE posts SET deleted=1 WHERE message_id=? '''\n",
    "            parsed = (message_id, )\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(sql, parsed)\n",
    "            conn.commit()\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scraping the soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(retries=5, backoff_factor=.1, session=None):\n",
    "    \"\"\" retry the request, backing off with longer rest each time\n",
    "    :param retries: number of retries\n",
    "    :param backoff_factor: each retry is longer by {backoff factor} * (2 ** ({number of total retries} - 1))\n",
    "    :param session: persist session across requests\n",
    "    :return session: session\n",
    "    \"\"\"\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(next_url):\n",
    "    \"\"\" get the soup from the url\n",
    "    :param next_url: string of next url to query\n",
    "    :return soup: soup of url html\n",
    "    :note: uses html5lib and not html because missing html returns errors\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res_next = requests_retry_session().get(next_url)\n",
    "    except:\n",
    "        return False\n",
    "    soup = BeautifulSoup(res_next.content, \"html5lib\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_posts(soup):\n",
    "    \"\"\" get each main list item from the page\n",
    "    :param soup: url's html\n",
    "    :return lis: list of threads from page\n",
    "    \"\"\"\n",
    "    ol = soup.find('ol', id=\"thread-list\")\n",
    "    lis = ol.find_all('li', recursive=False)\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parsing post text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ago(date_created, date_recorded):\n",
    "    \"\"\" if the post date created includes a relative\n",
    "        instead of absolute time (ago vs m-d-y), fix\n",
    "        and replace the time\n",
    "    :param date_created: the date the post was created\n",
    "    :param date_recorded: date recorded by the scraper\n",
    "    :return dc: date_created in datetime\n",
    "    \"\"\"\n",
    "    if \"hr\" in date_created:\n",
    "        if \"min\" in date_created:\n",
    "            l = re.findall(\"[0-9]+\", date_created)\n",
    "            dc = datetime.strptime(date_recorded, \"%m-%d-%Y %H:%M:%S\") - timedelta(hours=int(l[0]), minutes=int(l[1]))\n",
    "        else:\n",
    "            l = re.findall(\"[0-9]+\", date_created)[0]\n",
    "            dc = datetime.strptime(date_recorded, \"%m-%d-%Y %H:%M:%S\") - timedelta(hours=int(l))        \n",
    "    else:\n",
    "        l = re.findall(\"[0-9]+\", date_created)[0]\n",
    "        dc = datetime.strptime(date_recorded, \"%m-%d-%Y %H:%M:%S\") - timedelta(minutes=int(l))\n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date(date_created):\n",
    "    \"\"\" removes extra text from date\n",
    "    :param date_created: string of date\n",
    "    :return dc: stripped string of date\n",
    "    \"\"\"\n",
    "    dc = date_created.replace('posted ','')\n",
    "    dc = dc.replace(' in Tween/Teen','')\n",
    "    dc = dc.replace(' in Elementary','')\n",
    "    dc = dc.replace(' in Preschool','')\n",
    "    dc = dc.replace(' in Toddler','')\n",
    "    dc = dc.replace(' in Newborn','')\n",
    "    dc = dc.replace(' in Special Needs','')\n",
    "    dc = dc.replace(' in Expecting','')\n",
    "    dc = dc.replace(' in TTC','')\n",
    "    dc = dc.replace(' in Single Parents','')\n",
    "    dc = dc.replace(' in Weight Watchers','')\n",
    "    dc = dc.replace(' in YBM Feedback','')\n",
    "    dc = dc.replace(' in Boston','')\n",
    "    dc = dc.replace(' in Chicago','')\n",
    "    dc = dc.replace(' in Los Angeles','')\n",
    "    dc = dc.replace(' in New York City','')\n",
    "    dc = dc.replace(' in NYC Schools','')\n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_thread(conn, thread, subforum, date_recorded):\n",
    "    \"\"\" parse the thread url and date created\n",
    "    :param thread: input thread soup\n",
    "    :param conn: connection to db\n",
    "    :return url: url of the thread\n",
    "    :return date_created: date the thread was created\n",
    "    \"\"\"\n",
    "    url_find = thread.find(\"a\", text=re.compile(\"permalink\"))\n",
    "    if url_find:\n",
    "        url = url_find[\"href\"]\n",
    "        if 'https://www.youbemom.com' in url: # some urls scraped with site\n",
    "            url = url.replace('https://www.youbemom.com','')\n",
    "        date_created = thread.find('span', {'class' : 'meta date'}).get_text()\n",
    "        # if doesn't contain \"ago\", change time with strptime\n",
    "        if \"ago\" in date_created:\n",
    "            date_created = fix_ago(date_created, date_recorded)\n",
    "        else:\n",
    "            date_created = datetime.strptime(date_created, \"%m-%d-%Y %I:%M%p\")\n",
    "        if url_not_in_threads(conn, url):\n",
    "            write_to_threads(conn, url, subforum)\n",
    "    else:\n",
    "        print(\"didn't find url: \" + str(thread))\n",
    "        date_created = datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "    return date_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subforum(soup):\n",
    "    action = soup.find('form', {'id' : 'search'})\n",
    "    if action:\n",
    "        subforum = action['action']\n",
    "        subforum = subforum.replace(\"/forum/\",\"\")\n",
    "        return subforum\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" clean the text of extra spaces, new lines,\n",
    "        ellipses, and (more) text\n",
    "    :param text: input text\n",
    "    :return text: cleaned text\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"\\(more\\)\", \"\", text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    text = re.sub(\"\\.{3}\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_post_parent(soup, conn, family_id, date_recorded, subforum):\n",
    "    \"\"\" parse the list items into a format that can be\n",
    "        inserted into the database (top post in thread)\n",
    "    :param soup: input soup of parent post\n",
    "    :param conn: connection to db\n",
    "    :param date_recorded: date scraping the data\n",
    "    \"\"\"\n",
    "    title_html = soup.find(\"h1\")\n",
    "    title = title_html.get_text()\n",
    "    title = clean_text(title)\n",
    "    if title_html.has_attr('class') and \"removed\" in title_html['class']:\n",
    "        deleted = 1\n",
    "    else:\n",
    "        deleted = 0\n",
    "    message_id = title_html[\"id\"]\n",
    "    body_html = soup.find('div', {'class' : 'message', 'id' : \"p\" + message_id}, recursive=False)\n",
    "    if body_html:\n",
    "        body = body_html.get_text()\n",
    "        body = body.replace('log in or sign up to post a comment', '')\n",
    "        body = clean_text(body)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    date_created = soup.find('div', {'class' : 'date'}).get_text()\n",
    "    # if doesn't contain \"ago\", change time with strptime\n",
    "    if \"ago\" in date_created:\n",
    "        date_created = fix_ago(date_created, date_recorded)\n",
    "    else:\n",
    "        date_created = fix_date(date_created)\n",
    "        date_created = parse(date_created)\n",
    "    if message_not_in_posts(conn, message_id, deleted):\n",
    "        parsed = (family_id,message_id,\"\",date_recorded,date_created,title,body,subforum,deleted)\n",
    "        write_to_posts(parsed, conn)\n",
    "    return message_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_post_child(soup, conn, family_id, parent_id, date_recorded, subforum):\n",
    "    \"\"\" parse the list items into a format that can be\n",
    "        inserted into the database (child replys)\n",
    "    :param soup: input soup of child post\n",
    "    :param conn: connection to db\n",
    "    :param family_id: id of the family thread\n",
    "    :param parent_id: id of the parent post to this child\n",
    "    :param date_recorded: date scraping the data\n",
    "    :NOTE: unlike top post, must re.compile class because\n",
    "           it might be class='noskimwords reply removed'\n",
    "    \"\"\"\n",
    "    title_html = soup.find('span', {'class' : re.compile('noskimwords reply')})\n",
    "    title = title_html.get_text()\n",
    "    title = clean_text(title)\n",
    "    if title_html.has_attr('class') and \"removed\" in title_html['class']:\n",
    "        deleted = 1\n",
    "    else:\n",
    "        deleted = 0\n",
    "    message_id = title_html[\"id\"]\n",
    "    body_html = soup.find('div', {'class' : 'message', 'id' : \"p\" + message_id}, recursive=False)\n",
    "    if body_html:\n",
    "        body = body_html.get_text()\n",
    "        body = body.replace('log in or sign up to post a comment', '')\n",
    "        body = clean_text(body)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    date_created = soup.find('span', {'class' : 'meta date'}).get_text()\n",
    "    # if doesn't contain \"ago\", change time with strptime\n",
    "    if \"ago\" in date_created:\n",
    "        date_created = fix_ago(date_created, date_recorded)\n",
    "    else:\n",
    "        date_created = fix_date(date_created)\n",
    "        date_created = parse(date_created)\n",
    "    if message_not_in_posts(conn, message_id, deleted):\n",
    "        parsed = (family_id,message_id,parent_id,date_recorded,date_created,title,body,subforum,deleted)\n",
    "        write_to_posts(parsed, conn)\n",
    "    return message_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_children(children, conn, family_id, parent_id, date_recorded, subforum):\n",
    "    for child in children:\n",
    "        message_id = parse_post_child(child, conn, family_id, parent_id, date_recorded, subforum)\n",
    "        replies = child.find('ul')\n",
    "        if replies:\n",
    "            grandchildren = replies.find_all(\"li\", recursive=False)\n",
    "            search_children(grandchildren, conn, family_id, message_id, date_recorded, subforum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For looping through the forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_link():\n",
    "    soup = get_soup(\"https://www.youbemom.com/forum/toddler\")\n",
    "    if soup:\n",
    "        top_posts = get_top_posts(soup)\n",
    "        thread = top_posts[0]\n",
    "        url_find = thread.find(\"a\", text=re.compile(\"permalink\"))\n",
    "        url = url_find[\"href\"]\n",
    "        url = url.replace('https://www.youbemom.com/forum/permalink/','')\n",
    "        url = url.replace('/forum/permalink/','')\n",
    "        current = re.findall('[0-9]+', url)[0]\n",
    "        return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_threads(conn, path_db, subforum, earliest):\n",
    "    forum_url = \"https://www.youbemom.com/forum/\" + subforum\n",
    "    next_url = forum_url[:]\n",
    "    scraped_earliest = False\n",
    "    page = 1 # threads\n",
    "    print(\"subforum: \" + subforum)\n",
    "    while not scraped_earliest:\n",
    "        date_recorded = datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "        if page > 1000:\n",
    "            break\n",
    "        if page % 10 == 0:\n",
    "            print(\"page: \" + str(page))\n",
    "        soup = get_soup(next_url)\n",
    "        if soup:\n",
    "            top_posts = get_top_posts(soup)\n",
    "            # parse each top thread post for url\n",
    "            for thread in top_posts:\n",
    "                date_created = parse_thread(conn, thread, subforum, date_recorded)\n",
    "                scraped_earliest = date_created < earliest\n",
    "        else:\n",
    "            print(\"error page \" + str(page) + \" with url \" + url)\n",
    "        page += 1\n",
    "        next_url = forum_url + \"?pg=\" + str(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_link_threads(conn, path_db, earliest_link, last_link):\n",
    "    forum_url = \"https://www.youbemom.com/forum/permalink/\"\n",
    "    sql = \"\"\" SELECT MAX(id) FROM threads \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    max_id = cur.fetchone()[0]\n",
    "    if max_id:\n",
    "        next_id = int(max_id) + 1\n",
    "    else:\n",
    "        next_id = 1\n",
    "    print(next_id)\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time: \", current_time)\n",
    "    bad_urls = []\n",
    "    bad_ids = []\n",
    "    for post_num in range(earliest_link + next_id - 1, last_link + 1):\n",
    "        if next_id % 1000 == 0:\n",
    "            print(\"id: \" + str(next_id))\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(\"Current Time =\", current_time)\n",
    "        next_url = forum_url + str(post_num)\n",
    "        soup = get_soup(next_url)\n",
    "        if soup:\n",
    "            url = \"/forum/permalink/\" + str(post_num)\n",
    "            subforum = get_subforum(soup)\n",
    "            if subforum:\n",
    "                write_to_threads(conn, url, subforum)\n",
    "                row = [next_id, url]\n",
    "                bad = parse_row(conn, path_db, subforum, row)\n",
    "                bad_urls = bad_urls + bad['bad_url']\n",
    "                bad_ids = bad_ids + bad['bad_id']\n",
    "        next_id += 1\n",
    "    return {\"bad_urls\":bad_urls, \"bad_ids\":bad_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_posts(conn, path_db, subforum):\n",
    "    sql = \"\"\" SELECT * FROM threads WHERE subforum='{}' \"\"\".format(subforum)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    batch = 1 # posts\n",
    "    batch_size = 100\n",
    "    bad_urls = []\n",
    "    bad_ids = []\n",
    "    while True:\n",
    "        print(\"batch: \" + str(batch))\n",
    "        rows = cur.fetchmany(batch_size)\n",
    "        if not rows: break\n",
    "        for row in rows:\n",
    "            bad = parse_row(conn, path_db, subforum, row)\n",
    "            bad_urls = bad_urls + bad['bad_url']\n",
    "            bad_ids = bad_ids + bad['bad_id']\n",
    "        batch += 1\n",
    "    return {\"bad_urls\":bad_urls, \"bad_ids\":bad_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(conn, path_db, subforum, row):\n",
    "    date_recorded = datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "    family_id = row[0]\n",
    "    url = row[1]\n",
    "    bad_url = []\n",
    "    bad_id = []\n",
    "    if 'https://www.youbemom.com' not in url and 'http://www.youbemom.com' not in url:\n",
    "        url = 'https://www.youbemom.com' + url\n",
    "    soup = get_soup(url)\n",
    "    if soup:\n",
    "        message_id = parse_post_parent(soup, conn, family_id, date_recorded, subforum)\n",
    "        replies = soup.find('ul', {'id' : 'reply-list'})\n",
    "        if replies:\n",
    "            children = replies.find_all('li', recursive=False)\n",
    "            search_children(children, conn, family_id, message_id, date_recorded, subforum)\n",
    "    else:\n",
    "        print(\"connection error\")\n",
    "        print(\"could not get url: \" + url)\n",
    "        print(\"with family id: \" + str(family_id))\n",
    "        bad_url.append(url)\n",
    "        bad_id.append(family_id)\n",
    "    return {\"bad_url\":bad_url, \"bad_id\":bad_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[0]\n",
    "path_db = path_parent / \"database\" / \"youbemomTables.db\"\n",
    "path_db = str(path_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "The toddler forum is so much larger than the other forums so it is scraped separately (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_list = [\"special-needs\", \"newborn\", \"preschool\", \"elementary\", \"tween-teen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Thread URLs\n",
    "Connect to the database and create the tables.\n",
    "NOTE: Scrape from Permalink scrapes all posts 2018-01-01 to present\n",
    "NOTE: Scrape from Subforum scrapes listed subforums up to earliest date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(path_db)\n",
    "set_up_db(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape from Permalink\n",
    "This would take ~27-55 days to run. I split the links into 10 parts and ran them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# earliest_toddler = 9454477 # first toddler post in 2018\n",
    "# current_toddler = int(get_current_toddler())\n",
    "# bad = loop_toddler_threads(conn, path_db, earliest_toddler, current_toddler)\n",
    "# print(\"bad urls: \" + bad['bad_urls'])\n",
    "# print(\"bad ids: \" + bad['bad_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape from Subforum\n",
    "Run while loop until scraper reaches the earliest post in the listed subforums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subforum: special-needs\n",
      "page: 10\n",
      "page: 20\n",
      "page: 30\n",
      "page: 40\n",
      "page: 50\n",
      "page: 60\n",
      "page: 70\n",
      "page: 80\n",
      "page: 90\n",
      "page: 100\n",
      "page: 110\n",
      "page: 120\n",
      "page: 130\n",
      "page: 140\n",
      "page: 150\n",
      "page: 160\n",
      "page: 170\n",
      "page: 180\n",
      "page: 190\n",
      "page: 200\n",
      "page: 210\n",
      "page: 220\n",
      "page: 230\n",
      "page: 240\n",
      "page: 250\n",
      "page: 260\n",
      "page: 270\n",
      "page: 280\n",
      "page: 290\n",
      "page: 300\n",
      "page: 310\n",
      "page: 320\n",
      "page: 330\n",
      "page: 340\n",
      "page: 350\n",
      "page: 360\n",
      "page: 370\n",
      "page: 380\n",
      "page: 390\n",
      "page: 400\n",
      "page: 410\n",
      "page: 420\n",
      "page: 430\n",
      "page: 440\n",
      "page: 450\n",
      "page: 460\n",
      "page: 470\n",
      "page: 480\n",
      "page: 490\n",
      "page: 500\n",
      "page: 510\n",
      "page: 520\n",
      "page: 530\n",
      "page: 540\n",
      "page: 550\n",
      "page: 560\n",
      "page: 570\n",
      "page: 580\n",
      "page: 590\n",
      "page: 600\n",
      "page: 610\n",
      "page: 620\n",
      "page: 630\n",
      "page: 640\n",
      "page: 650\n",
      "page: 660\n",
      "page: 670\n",
      "page: 680\n",
      "page: 690\n",
      "page: 700\n",
      "page: 710\n",
      "page: 720\n",
      "page: 730\n",
      "page: 740\n",
      "page: 750\n",
      "page: 760\n",
      "page: 770\n",
      "page: 780\n",
      "page: 790\n",
      "page: 800\n",
      "page: 810\n",
      "page: 820\n",
      "page: 830\n",
      "page: 840\n",
      "page: 850\n",
      "page: 860\n",
      "page: 870\n",
      "page: 880\n",
      "page: 890\n",
      "page: 900\n",
      "page: 910\n",
      "page: 920\n",
      "page: 930\n",
      "page: 940\n",
      "page: 950\n",
      "page: 960\n",
      "page: 970\n",
      "page: 980\n",
      "page: 990\n",
      "page: 1000\n",
      "subforum: newborn\n",
      "page: 10\n",
      "subforum: preschool\n",
      "page: 10\n",
      "subforum: elementary\n",
      "page: 10\n",
      "page: 20\n",
      "subforum: tween-teen\n",
      "page: 10\n",
      "page: 20\n",
      "page: 30\n"
     ]
    }
   ],
   "source": [
    "earliest = datetime(2014, 1, 1, 0, 0, 0)\n",
    "for subforum in forum_list:\n",
    "    loop_threads(conn, path_db, subforum, earliest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Thread Posts\n",
    "For subforum in the forum and for urls in the threads table in that subforum, pulls a batch of 100 urls and scrapes each post in the thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n",
      "batch: 10\n",
      "batch: 11\n",
      "batch: 12\n",
      "batch: 13\n",
      "batch: 14\n",
      "batch: 15\n",
      "batch: 16\n",
      "batch: 17\n",
      "batch: 18\n",
      "batch: 19\n",
      "batch: 20\n",
      "batch: 21\n",
      "batch: 22\n",
      "batch: 23\n",
      "batch: 24\n",
      "batch: 25\n",
      "batch: 26\n",
      "batch: 27\n",
      "batch: 28\n",
      "batch: 29\n",
      "batch: 30\n",
      "batch: 31\n",
      "batch: 32\n",
      "batch: 33\n",
      "batch: 34\n",
      "batch: 35\n",
      "batch: 36\n",
      "batch: 37\n",
      "batch: 38\n",
      "batch: 39\n",
      "batch: 40\n",
      "batch: 41\n",
      "batch: 42\n",
      "batch: 43\n",
      "batch: 44\n",
      "batch: 45\n",
      "batch: 46\n",
      "batch: 47\n",
      "batch: 48\n",
      "batch: 49\n",
      "batch: 50\n",
      "batch: 51\n",
      "batch: 52\n",
      "batch: 53\n",
      "batch: 54\n",
      "batch: 55\n",
      "batch: 56\n",
      "batch: 57\n",
      "batch: 58\n",
      "batch: 59\n",
      "batch: 60\n",
      "batch: 61\n",
      "batch: 62\n",
      "batch: 63\n",
      "batch: 64\n",
      "batch: 65\n",
      "batch: 66\n",
      "batch: 67\n",
      "batch: 68\n",
      "batch: 69\n",
      "batch: 70\n",
      "batch: 71\n",
      "batch: 72\n",
      "batch: 73\n",
      "batch: 74\n",
      "batch: 75\n",
      "batch: 76\n",
      "batch: 77\n",
      "batch: 78\n",
      "batch: 79\n",
      "connection error\n",
      "could not get url: https://www.youbemom.com/forum/permalink/9079199/moms-of-kids-with-asd-who-are-older-than-say-5-7-what-do-you\n",
      "with family id: 7818\n",
      "connection error\n",
      "could not get url: https://www.youbemom.com/forum/permalink/9079068/seeking-feedback-on-nyc-in-home-behaviorists-please\n",
      "with family id: 7819\n",
      "connection error\n",
      "could not get url: https://www.youbemom.com/forum/permalink/9078963/adderall-vs-vyvanse\n",
      "with family id: 7820\n",
      "batch: 80\n",
      "batch: 81\n",
      "batch: 82\n",
      "batch: 83\n",
      "batch: 84\n",
      "batch: 85\n",
      "batch: 86\n",
      "batch: 87\n",
      "batch: 88\n",
      "batch: 89\n",
      "batch: 90\n",
      "batch: 91\n",
      "batch: 92\n",
      "batch: 93\n",
      "batch: 94\n",
      "batch: 95\n",
      "batch: 96\n",
      "batch: 97\n",
      "batch: 98\n",
      "batch: 99\n",
      "batch: 100\n",
      "batch: 101\n",
      "batch: 102\n",
      "batch: 103\n",
      "batch: 104\n",
      "batch: 105\n",
      "batch: 106\n",
      "batch: 107\n",
      "batch: 108\n",
      "batch: 109\n",
      "batch: 110\n",
      "batch: 111\n",
      "batch: 112\n",
      "batch: 113\n",
      "batch: 114\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/8530395/i-need-advice-sn-moms-you-may-remember-me-from-last-summer-im-the-one\n",
      "with family id: 11398\n",
      "batch: 115\n",
      "batch: 116\n",
      "batch: 117\n",
      "batch: 118\n",
      "batch: 119\n",
      "batch: 120\n",
      "batch: 121\n",
      "batch: 122\n",
      "batch: 123\n",
      "batch: 124\n",
      "batch: 125\n",
      "batch: 126\n",
      "batch: 127\n",
      "batch: 128\n",
      "batch: 129\n",
      "batch: 130\n",
      "batch: 131\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/8339840/reposting-i-need-help-please\n",
      "with family id: 13017\n",
      "batch: 132\n",
      "batch: 133\n",
      "batch: 134\n",
      "batch: 135\n",
      "batch: 136\n",
      "batch: 137\n",
      "batch: 138\n",
      "batch: 139\n",
      "batch: 140\n",
      "batch: 141\n",
      "batch: 142\n",
      "batch: 143\n",
      "batch: 144\n",
      "batch: 145\n",
      "batch: 146\n",
      "batch: 147\n",
      "batch: 148\n",
      "batch: 149\n",
      "batch: 150\n",
      "batch: 151\n",
      "batch: 152\n",
      "batch: 153\n",
      "batch: 154\n",
      "batch: 155\n",
      "batch: 156\n",
      "batch: 157\n",
      "batch: 158\n",
      "batch: 159\n",
      "batch: 160\n",
      "batch: 161\n",
      "batch: 162\n",
      "batch: 163\n",
      "batch: 164\n",
      "batch: 165\n",
      "batch: 166\n",
      "batch: 167\n",
      "batch: 168\n",
      "batch: 169\n",
      "batch: 170\n",
      "batch: 171\n",
      "batch: 172\n",
      "batch: 173\n",
      "batch: 174\n",
      "batch: 175\n",
      "batch: 176\n",
      "batch: 177\n",
      "batch: 178\n",
      "batch: 179\n",
      "batch: 180\n",
      "batch: 181\n",
      "batch: 182\n",
      "batch: 183\n",
      "batch: 184\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7687586/i-think-my-7-yo-ds-is-either-having-seizures-or-something-anyone-else\n",
      "with family id: 18366\n",
      "batch: 185\n",
      "batch: 186\n",
      "batch: 187\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7656587/i-have-a-6-year-old-dd-who-is-likely-to-be-on-the-spectrum-i-feel-like\n",
      "with family id: 18648\n",
      "batch: 188\n",
      "batch: 189\n",
      "batch: 190\n",
      "batch: 191\n",
      "batch: 192\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7608534/any-teachers-on-how-can-i-tell-what-my-kids-teacher-is-just-saying-and\n",
      "with family id: 19157\n",
      "batch: 193\n",
      "batch: 194\n",
      "batch: 195\n",
      "batch: 196\n",
      "batch: 197\n",
      "batch: 198\n",
      "batch: 199\n",
      "batch: 200\n",
      "batch: 201\n",
      "batch: 202\n",
      "batch: 203\n",
      "batch: 204\n",
      "batch: 205\n",
      "batch: 206\n",
      "batch: 207\n",
      "batch: 208\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7457202/i-just-found-these-jeans-for-those-of-us-with-boys-who-are-big-and-hav\n",
      "with family id: 20751\n",
      "batch: 209\n",
      "batch: 210\n",
      "batch: 211\n",
      "batch: 212\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7428290/is-this-psychological-ds-8-has-been-having-a-rough-couple-of-months-we\n",
      "with family id: 21142\n",
      "batch: 213\n",
      "batch: 214\n",
      "batch: 215\n",
      "batch: 216\n",
      "batch: 217\n",
      "batch: 218\n",
      "batch: 219\n",
      "batch: 220\n",
      "batch: 221\n",
      "batch: 222\n",
      "batch: 223\n",
      "batch: 224\n",
      "batch: 225\n",
      "batch: 226\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7286508/guess-what-we-set-up-10yo-dds-tablet-for-texting-to-her-email-since-he\n",
      "with family id: 22550\n",
      "batch: 227\n",
      "batch: 228\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7269181/arg-have-posted-before-about-my-6yo-dd-starting-in-k-i-noticed-sensory\n",
      "with family id: 22798\n",
      "batch: 229\n",
      "batch: 230\n",
      "batch: 231\n",
      "batch: 232\n",
      "batch: 233\n",
      "batch: 234\n",
      "batch: 235\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7207727/does-anyone-want-to-help-me-with-private-schools-im-trying-to-decide-i\n",
      "with family id: 23496\n",
      "batch: 236\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7202009/ds-was-evaluated-and-dxd-with-adhd-at-age-4-his-full-scale-iq-at-that\n",
      "with family id: 23562\n",
      "batch: 237\n",
      "batch: 238\n",
      "batch: 239\n",
      "batch: 240\n",
      "batch: 241\n",
      "connection error\n",
      "could not get url: https://www.youbemom.comhttp://www.youbemom.com/forum/permalink/7164130/please-help-about-to-lose-it-just-got-back-from-dds-k-conference-teach\n",
      "with family id: 24011\n",
      "batch: 242\n",
      "batch: 243\n",
      "batch: 244\n",
      "batch: 245\n",
      "batch: 246\n",
      "batch: 247\n",
      "batch: 248\n",
      "batch: 249\n",
      "batch: 250\n",
      "batch: 251\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n"
     ]
    }
   ],
   "source": [
    "bad_urls = []\n",
    "bad_ids = []\n",
    "for subforum in forum_list:\n",
    "    bad = loop_posts(conn, path_db, subforum)\n",
    "    bad_urls = bad_urls + bad[\"bad_urls\"]\n",
    "    bad_ids = bad_ids + bad[\"bad_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix bad urls. Three worked the second time and twelve were http not https."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n",
      "{'bad_url': [], 'bad_id': []}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bad_ids)):\n",
    "    sql = ''' SELECT * FROM threads WHERE id=?'''\n",
    "    parsed = (bad_ids[i],)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    row = cur.fetchone()\n",
    "    if row:\n",
    "        bad = parse_row(conn, path_db, subforum, row)\n",
    "        print(bad)\n",
    "    else:\n",
    "        print(\"did not get row {} from db\".format(bad_ids[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
