{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Forum Data\n",
    "This notebook scrapes the Youbemom special needs forum and inserts the results into a SQLite database\n",
    "\n",
    "## Data Sources\n",
    "- Youbemom forum: https://www.youbemom.com/forum/special-needs\n",
    "\n",
    "## Changes\n",
    "- 2020-08-11: Started project\n",
    "- 2020-08-18: Updated forum crawl\n",
    "- 2020-08-22: Updated database structure\n",
    "\n",
    "## Database Structure\n",
    "- threads\n",
    " - id: automatically assigned\n",
    " - url: url of top post\n",
    "- posts\n",
    " - id: automatically assigned\n",
    " - family_id: thread->id\n",
    " - message_id: the unique id of the message from the html\n",
    " - parent_id: id of post this post is responding to, 0 if top post\n",
    " - date_recorded: date the data is fetched\n",
    " - date_created: date the data was created\n",
    " - title: title of the post\n",
    " - body: body of the post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from pathlib import Path\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "For accessing the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by the db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "    except Error as err:\n",
    "        print(err)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_db(conn):\n",
    "    \"\"\" if the database exists, drop it and create a\n",
    "        SQLite database for the results\n",
    "    :param conn: database connection\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.executescript('''\n",
    "        CREATE TABLE IF NOT EXISTS threads (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "            url TEXT\n",
    "        );\n",
    "        CREATE TABLE IF NOT EXISTS posts (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "            family_id INTEGER,\n",
    "            message_id TEXT,\n",
    "            parent_id INTEGER,\n",
    "            date_recorded TEXT,\n",
    "            date_created TEXT,\n",
    "            title TEXT,\n",
    "            body TEXT\n",
    "        );\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_threads(url, conn):\n",
    "    \"\"\" inserts the parsed data into the threads table\n",
    "    :param parsed: a tuple of the parsed data\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    sql = ''' INSERT INTO threads(url)\n",
    "    VALUES(?) '''\n",
    "    parsed = (url,)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_not_in_threads(url, conn):\n",
    "    \"\"\" checks to see if scraped url is already \n",
    "        in the threads database\n",
    "    :return: True if not scraped, False if already scraped\n",
    "    \"\"\"\n",
    "    sql = ''' SELECT id FROM threads WHERE url=?'''\n",
    "    parsed = (url,)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    result = cur.fetchone()\n",
    "    if result:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_posts(parsed, conn):\n",
    "    \"\"\" inserts the parsed data into the posts table\n",
    "    :param parsed: a tuple of the parsed data\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    sql = ''' INSERT INTO posts(family_id,message_id,parent_id,date_recorded,date_created,title,body)\n",
    "    VALUES(?,?,?,?,?,?,?) '''\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_not_in_posts(message_id, conn):\n",
    "    \"\"\" checks to see if the message_id is already \n",
    "        in the posts database, used to not overwrite\n",
    "        posts with the deleted message when re-running\n",
    "        the scraping of the posts\n",
    "    :return: True if not scraped, False if already scraped\n",
    "    \"\"\"\n",
    "    sql = ''' SELECT id FROM posts WHERE message_id=?'''\n",
    "    parsed = (message_id,)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, parsed)\n",
    "    result = cur.fetchone()\n",
    "    if result:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scraping the soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(retries=5, backoff_factor=10, session=None):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(next_url):\n",
    "    \"\"\" get the soup from the url\n",
    "    :param next_url: string of next url to query\n",
    "    :return: soup of url html\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res_next = requests_retry_session().get(next_url)\n",
    "    except:\n",
    "        return False\n",
    "    soup = BeautifulSoup(res_next.content, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_posts(soup):\n",
    "    \"\"\" get each main list item from the page\n",
    "    :param soup: url's html\n",
    "    :return lis: list of threads from page\n",
    "    \"\"\"\n",
    "    ol = soup.find('ol', id=\"thread-list\")\n",
    "    lis = ol.find_all('li', recursive=False)\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parsing post text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ago(date_created, date_recorded):\n",
    "    \"\"\" if the post date created includes a relative\n",
    "        instead of absolute time (ago vs m-d-y), fix\n",
    "        and replace the time\n",
    "    :param date_created: the date the post was created\n",
    "    :param date_recorded: date recorded by the scraper\n",
    "    :return: date_created in datetime\n",
    "    \"\"\"\n",
    "    if \"hr\" in date_created:\n",
    "        if \"min\" in date_created:\n",
    "            l = re.findall(\"[0-9]+\", date_created)\n",
    "            dc = datetime.strptime(date_recorded, \"%m-%d-%Y %H:%M:%S\") - timedelta(hours=int(l[0]), minutes=int(l[1]))\n",
    "        else:\n",
    "            l = re.findall(\"[0-9]+\", date_created)[0]\n",
    "            dc = datetime.strptime(date_recorded, \"%m-%d-%Y %H:%M:%S\") - timedelta(hours=int(l))        \n",
    "    else:\n",
    "        l = re.findall(\"[0-9]+\", date_created)[0]\n",
    "        dc = datetime.strptime(date_recorded, \"%m-%d-%Y %H:%M:%S\") - timedelta(minutes=int(l))\n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date(date_created):\n",
    "    dc = date_created.replace('posted ','')\n",
    "    dc = dc.replace(' in Special Needs','')\n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_thread(thread, conn):\n",
    "    \"\"\" parse the thread url and date created\n",
    "    :param thread: input thread soup\n",
    "    :param conn: connection to db\n",
    "    :return url: url of the thread\n",
    "    :return date_created: date the thread was created\n",
    "    \"\"\"\n",
    "    url = thread.find(\"a\", text=re.compile(\"permalink\"))[\"href\"]\n",
    "    if 'https://www.youbemom.com' in url: # some urls scraped with site\n",
    "        url = url.replace('https://www.youbemom.com','')\n",
    "    date_created = thread.find('span', {'class' : 'meta date'}).get_text()\n",
    "    # if doesn't contain \"ago\", change time with strptime\n",
    "    if \"ago\" in date_created:\n",
    "        date_created = fix_ago(date_created, date_recorded)\n",
    "    else:\n",
    "        date_created = datetime.strptime(date_created, \"%m-%d-%Y %I:%M%p\")\n",
    "    if url_not_in_threads(url, conn):\n",
    "        write_to_threads(url, conn)\n",
    "    return date_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" clean the text of extra spaces, new lines,\n",
    "        ellipses, and (more) text\n",
    "    :param text: input text\n",
    "    :return text: cleaned text\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"\\(more\\)\", \"\", text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    text = re.sub(\"\\.{3}\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_post_parent(soup, conn, family_id, date_recorded):\n",
    "    \"\"\" parse the list items into a format that can be\n",
    "        inserted into the database (top post in thread)\n",
    "    :param soup: input soup of parent post\n",
    "    :param conn: connection to db\n",
    "    :param date_recorded: date scraping the data\n",
    "    \"\"\"\n",
    "    title_html = soup.find(\"h1\")\n",
    "    title = title_html.get_text()\n",
    "    title = clean_text(title)\n",
    "    message_id = title_html[\"id\"]\n",
    "    body_html = soup.find('div', {'class' : 'message', 'id' : \"p\" + message_id}, recursive=False)\n",
    "    if body_html:\n",
    "        body = body_html.get_text()\n",
    "        body = body.replace('log in or sign up to post a comment', '')\n",
    "        body = clean_text(body)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    date_created = soup.find('div', {'class' : 'date'}).get_text()\n",
    "    # if doesn't contain \"ago\", change time with strptime\n",
    "    if \"ago\" in date_created:\n",
    "        date_created = fix_ago(date_created, date_recorded)\n",
    "    else:\n",
    "        date_created = fix_date(date_created)\n",
    "        date_created = parse(date_created)\n",
    "    if message_not_in_posts(message_id, conn):\n",
    "        parsed = (family_id,message_id,\"\",date_recorded,date_created,title,body)\n",
    "        write_to_posts(parsed, conn)\n",
    "    return message_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_post_child(soup, conn, family_id, parent_id, date_recorded):\n",
    "    \"\"\" parse the list items into a format that can be\n",
    "        inserted into the database (child replys)\n",
    "    :param soup: input soup of child post\n",
    "    :param conn: connection to db\n",
    "    :param family_id: id of the family thread\n",
    "    :param parent_id: id of the parent post to this child\n",
    "    :param date_recorded: date scraping the data\n",
    "    :NOTE: unlike top post, must re.compile class because\n",
    "           it might be class='noskimwords reply removed'\n",
    "    \"\"\"\n",
    "    title_html = soup.find('span', {'class' : re.compile('noskimwords reply')})\n",
    "    title = title_html.get_text()\n",
    "    title = clean_text(title)\n",
    "    message_id = title_html[\"id\"]\n",
    "    body_html = soup.find('div', {'class' : 'message', 'id' : \"p\" + message_id}, recursive=False)\n",
    "    if body_html:\n",
    "        body = body_html.get_text()\n",
    "        body = body.replace('log in or sign up to post a comment', '')\n",
    "        body = clean_text(body)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    date_created = soup.find('span', {'class' : 'meta date'}).get_text()\n",
    "    # if doesn't contain \"ago\", change time with strptime\n",
    "    if \"ago\" in date_created:\n",
    "        date_created = fix_ago(date_created, date_recorded)\n",
    "    else:\n",
    "        date_created = fix_date(date_created)\n",
    "        date_created = parse(date_created)\n",
    "    if message_not_in_posts(message_id, conn):\n",
    "        parsed = (family_id,message_id,parent_id,date_recorded,date_created,title,body)\n",
    "        write_to_posts(parsed, conn)\n",
    "    return message_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_children(children, conn, family_id, parent_id, date_recorded):\n",
    "    for child in children:\n",
    "        message_id = parse_post_child(child, conn, family_id, parent_id, date_recorded)\n",
    "        replies = child.find('ul')\n",
    "        if replies:\n",
    "            grandchildren = replies.find_all(\"li\", recursive=False)\n",
    "            search_children(grandchildren, conn, family_id, message_id, date_recorded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[0]\n",
    "path_db = path_parent / \"database\" / \"youbemomTables.db\"\n",
    "path_db = str(path_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_url = \"https://www.youbemom.com/forum/special-needs\"\n",
    "next_url = forum_url[:]\n",
    "earliest = datetime(2019, 1, 1, 0, 0, 0)\n",
    "scraped_earliest = False\n",
    "page = 1 # threads\n",
    "batch = 1 # posts\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Thread URLs\n",
    "Connect to the database and create the tables. NOTE: checks to see if a thread url has already been collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(path_db)\n",
    "set_up_db(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run while loop until scraper reaches the earliest date I want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 10\n",
      "page: 20\n",
      "page: 30\n",
      "page: 40\n",
      "page: 50\n",
      "page: 60\n",
      "page: 70\n",
      "page: 80\n",
      "page: 90\n",
      "page: 100\n"
     ]
    }
   ],
   "source": [
    "while not scraped_earliest:\n",
    "    date_recorded = datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "    if page % 10 == 0:\n",
    "        print(\"page: \" + str(page))\n",
    "    soup = get_soup(next_url)\n",
    "    if soup:\n",
    "        top_posts =  get_top_posts(soup)\n",
    "        # parse each top thread post for url\n",
    "        for thread in top_posts:\n",
    "            date_created = parse_thread(thread, conn)\n",
    "            scraped_earliest = date_created < earliest\n",
    "    else:\n",
    "        print(\"error page \" + str(page) + \" with url \" + url)\n",
    "    page += 1\n",
    "    next_url = forum_url + \"?pg=\" + str(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Thread Posts\n",
    "Select all from the threads table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = ''' SELECT * FROM threads '''\n",
    "cur = conn.cursor()\n",
    "cur.execute(sql)\n",
    "bad_urls = []\n",
    "bad_ids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For urls in the threads table, pulls a batch of 100 urls and scrapes each post in the thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n",
      "batch: 10\n",
      "batch: 11\n",
      "batch: 12\n",
      "batch: 13\n",
      "batch: 14\n",
      "batch: 15\n",
      "batch: 16\n",
      "batch: 17\n",
      "batch: 18\n",
      "batch: 19\n",
      "batch: 20\n",
      "batch: 21\n",
      "batch: 22\n",
      "batch: 23\n",
      "batch: 24\n",
      "batch: 25\n",
      "batch: 26\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"batch: \" + str(batch))\n",
    "    rows = cur.fetchmany(batch_size)\n",
    "    if not rows: break\n",
    "    for row in rows:\n",
    "        date_recorded = datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "        family_id = row[0]\n",
    "        url = row[1]\n",
    "        if 'https://www.youbemom.com' not in url:\n",
    "            url = 'https://www.youbemom.com' + url\n",
    "        soup = get_soup(url)\n",
    "        if soup:\n",
    "            message_id = parse_post_parent(soup, conn, family_id, date_recorded)\n",
    "            replies = soup.find('ul', {'id' : 'reply-list'})\n",
    "            if replies:\n",
    "                children = replies.find_all('li', recursive=False)\n",
    "                search_children(children, conn, family_id, message_id, date_recorded)\n",
    "        else:\n",
    "            print(\"connection error\")\n",
    "            print(\"could not get url: \" + url)\n",
    "            print(\"with family id: \" + str(family_id))\n",
    "            bad_urls.append(url)\n",
    "            bad_ids.append(family_id)\n",
    "    batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
