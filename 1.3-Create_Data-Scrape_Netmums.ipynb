{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Forum Data\n",
    "This notebook scrapes the Netmums and inserts the results into a SQLite database\n",
    "\n",
    "## Data Sources\n",
    "- Netmums forum: https://www.netmums.com/coffeehouse/\n",
    "\n",
    "## Changes\n",
    "- 2020-01-30: Created\n",
    "- 2020-02-02: Added thread scraping\n",
    "- 2020-02-05: Moved functions to netmums.py\n",
    "\n",
    "## TODO\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from netmums import *\n",
    "from scraping import *\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_db = str(path_parent / \"database\" / \"netmums-merged.db\")\n",
    "path_groups = str(path_parent / \"scripts\" / \"netmums-group_{}.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Forum List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of Netmum forum pages in the db. Skip if already created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(path_db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='forums' ''')\n",
    "if cur.fetchone()[0] == 0:\n",
    "    set_up_merged_db(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur.execute(''' SELECT count(id) FROM forums ''')\n",
    "if cur.fetchone()[0] == 0:\n",
    "    url = \"https://www.netmums.com/coffeehouse/\"\n",
    "    soup = get_soup(url)\n",
    "    forums = soup.find_all(\"div\", {\"class\": \"left\"})\n",
    "    for forum in forums:\n",
    "        if forum_link := forum.find('a', href=True):\n",
    "            parsed = (forum_link['href'].strip(), forum_link.get_text().strip())\n",
    "            sql = '''\n",
    "                INSERT INTO forums(url,name)\n",
    "                VALUES(?,?)\n",
    "            '''\n",
    "            cur.execute(sql, parsed)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subforums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(''' SELECT count(id) FROM subforums ''')\n",
    "if cur.fetchone()[0] == 0:\n",
    "    cur.execute(''' SELECT id, url FROM forums ''')\n",
    "    rows = cur.fetchall()\n",
    "    for row in rows:\n",
    "        (forum_id, url) = row\n",
    "        soup = get_soup(url)\n",
    "        for subforum in soup.find_all(\"a\", {\"class\": \"cCatTopic\"}):\n",
    "            parsed = (subforum['href'].strip(), subforum.get_text().strip(),forum_id)\n",
    "            sql = '''\n",
    "                INSERT INTO subforums(url,name,forum_id)\n",
    "                VALUES(?,?,?)\n",
    "            '''\n",
    "            cur.execute(sql, parsed)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur.execute(''' SELECT count(id) FROM threads ''')\n",
    "if cur.fetchone()[0] == 0:\n",
    "    cur.execute(''' SELECT id, url FROM subforums ''')\n",
    "    rows = cur.fetchall()\n",
    "    for row in tqdm(rows):\n",
    "        scrape_threads(cur, row)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts\n",
    "Create individual scraper files for each chunk of thread urls. Run with: ```nohup python3 netmums-group_xx.py > output_xx.txt &```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posts can have different formatting in different situations, which leads to problems with parsing:\n",
    "1. Must parse posts with citations to other posts differently than those without citations\n",
    "2. Long urls are shortened when posted directly\n",
    "3. WYSIWYG emojis must be translated from image to text\n",
    "4. Skip the first post on each page after the first, it is duplicated from the last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be restart automatically. It deletes the last record collected in threads and starts rescraping that thread_id (if one exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(''' SELECT max(id) FROM threads ''')\n",
    "max_id = cur.fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups = 10\n",
    "size = round(max_id / n_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\n",
    "## Imports\n",
    "\n",
    "from netmums import *\n",
    "from scraping import *\n",
    "from pathlib import Path\n",
    "\n",
    "## File Locations\n",
    "\n",
    "p = Path.cwd()\n",
    "path_parent = p.parents[0]\n",
    "path_db_parent = str(path_parent / \"database\" / \"netmums-merged.db\")\n",
    "path_db_child = str(path_parent / \"database\" / \"netmums{0}.db\")\n",
    "\n",
    "## Connect to the database and create the tables.\n",
    "\n",
    "### find max id in child db posts\n",
    "conn = create_connection(path_db_child)\n",
    "set_up_posts_db(conn)\n",
    "cur = conn.cursor()\n",
    "cur.execute(''' SELECT MAX(thread_id) FROM posts ''')\n",
    "max_thread = cur.fetchone()[0]\n",
    "if max_thread == None:\n",
    "    first = {1}\n",
    "else: # restart scraping from last complete thread id\n",
    "    cur.execute(''' DELETE FROM posts WHERE thread_id=?''', (max_thread,))\n",
    "    conn.commit()\n",
    "    first = max_thread\n",
    "conn.close()\n",
    "\n",
    "### select rows from parent db threads\n",
    "conn = create_connection(path_db_parent)\n",
    "cur = conn.cursor()\n",
    "cur.execute(''' SELECT id, url FROM threads WHERE id>=? AND id<={2} ''', (first,))\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "\n",
    "### connect back to child db\n",
    "conn = create_connection(path_db_child)\n",
    "set_up_posts_db(conn)\n",
    "\n",
    "## Scrape threads\n",
    "\n",
    "for row in rows:\n",
    "    scrape_posts(conn, row)\n",
    "\n",
    "conn.close()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in range(n_groups):\n",
    "    first = 1 + (size * g)\n",
    "    if g == (n_groups - 1):\n",
    "        last = max_id\n",
    "    else:\n",
    "        last = (size * (g + 1))\n",
    "    if g + 1 < 10:\n",
    "        num = \"0{}\".format(g + 1)\n",
    "    else:\n",
    "        num = \"{}\".format(g + 1)\n",
    "    with open(path_groups.format(num), 'w') as w:\n",
    "        w.write(text.format(num, first, last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "def doctype(soup):\n",
    "    items = [item for item in soup.contents if isinstance(item, bs4.Doctype)]\n",
    "    return items[0] if items else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
